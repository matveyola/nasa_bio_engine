import requests
import pandas as pd
import time
import re
import os
import json
from urllib.parse import urlparse
import xml.etree.ElementTree as ET
from io import StringIO
import html

class NCBIArticleExtractor:
    def __init__(self, email, max_retries=3, delay=0.34):
        self.email = email
        self.max_retries = max_retries
        self.delay = delay
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
        
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'restricted_skipped': 0,
            'failed': 0
        }
    
    def extract_pmc_id(self, url):
        """–í–∏—Ç—è–≥–Ω–µ–Ω–Ω—è PMC ID –∑ URL"""
        if not url or pd.isna(url):
            return None
        
        url = str(url).strip()
        
        patterns = [
            r'PMC(\d+)',
            r'/articles/PMC(\d+)',
            r'pmc=PMC(\d+)',
            r'https?://[^/]+/pmc/articles/PMC(\d+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url, re.IGNORECASE)
            if match:
                pmc_id = f"PMC{match.group(1)}"
                return pmc_id
        
        return None
    
    def check_article_restrictions(self, xml_content):
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –æ–±–º–µ–∂–µ–Ω—å –¥–æ—Å—Ç—É–ø—É –¥–æ —Å—Ç–∞—Ç—Ç—ñ"""
        if "does not allow downloading of the full text" in xml_content:
            return True
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–∑–º—ñ—Ä—É –∫–æ–Ω—Ç–µ–Ω—Ç—É (–¥—É–∂–µ –º–∞–ª–∏–π —Ä–æ–∑–º—ñ—Ä –º–æ–∂–µ –æ–∑–Ω–∞—á–∞—Ç–∏ –æ–±–º–µ–∂–µ–Ω–∏–π –¥–æ—Å—Ç—É–ø)
        text_only = re.sub(r'<[^>]+>', ' ', xml_content)
        text_only = re.sub(r'\s+', ' ', text_only).strip()
        if len(text_only) < 5000:  # –î—É–∂–µ –º–∞–ª–æ —Ç–µ–∫—Å—Ç—É –¥–ª—è –Ω–∞—É–∫–æ–≤–æ—ó —Å—Ç–∞—Ç—Ç—ñ
            return True
        
        return False
    
    def get_article_via_api(self, pmc_id):
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç—Ç—ñ —á–µ—Ä–µ–∑ API"""
        for attempt in range(self.max_retries):
            try:
                params = {
                    'db': 'pmc',
                    'id': pmc_id.replace('PMC', ''),
                    'retmode': 'xml',
                    'rettype': 'full'
                }
                
                headers = {
                    'User-Agent': f'NCBI_API_Client/1.0 ({self.email})',
                    'Accept': 'application/xml'
                }
                
                response = requests.get(
                    f"{self.base_url}/efetch.fcgi",
                    params=params,
                    headers=headers,
                    timeout=30
                )
                
                if response.status_code == 200:
                    return response.text
                elif response.status_code == 429:
                    wait_time = (2 ** attempt) * self.delay
                    time.sleep(wait_time)
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ —Å–ø—Ä–æ–±–∏ {attempt + 1}: {e}")
            
            if attempt < self.max_retries - 1:
                time.sleep(self.delay)
        
        return None
    
    def get_article_metadata(self, pmc_id):
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö —Å—Ç–∞—Ç—Ç—ñ"""
        try:
            params = {
                'db': 'pmc',
                'id': pmc_id.replace('PMC', ''),
                'retmode': 'json'
            }
            
            response = requests.get(f"{self.base_url}/esummary.fcgi", params=params, timeout=15)
            if response.status_code == 200:
                data = response.json()
                result = data.get('result', {})
                article_info = result.get(pmc_id, {})
                
                return {
                    'title': article_info.get('title', ''),
                    'authors': ', '.join(article_info.get('authors', [])),
                    'journal': article_info.get('fulljournalname', ''),
                    'pub_date': article_info.get('pubdate', ''),
                    'doi': article_info.get('elocationid', ''),
                    'pmcid': pmc_id
                }
        except Exception:
            pass
        
        return {
            'title': '',
            'authors': '',
            'journal': '',
            'pub_date': '',
            'doi': '',
            'pmcid': pmc_id
        }
    
    def extract_element_text(self, element):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–µ –≤–∏—Ç—è–≥–Ω–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ XML –µ–ª–µ–º–µ–Ω—Ç–∞"""
        if element is None:
            return ""
        
        texts = []
        
        if element.text and element.text.strip():
            texts.append(element.text.strip())
        
        for child in element:
            child_text = self.extract_element_text(child)
            if child_text:
                texts.append(child_text)
            
            if child.tail and child.tail.strip():
                texts.append(child.tail.strip())
        
        return ' '.join(texts)
    
    def parse_article_content(self, xml_content, pmc_id):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤–º—ñ—Å—Ç—É —Å—Ç–∞—Ç—Ç—ñ –∑ XML"""
        try:
            root = ET.fromstring(xml_content)
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_elem = root.find('.//article-title')
            title = self.extract_element_text(title_elem) if title_elem is not None else ""
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∞–±—Å—Ç—Ä–∞–∫—Ç—É
            abstract_elems = root.findall('.//abstract')
            abstract_texts = []
            for abstract in abstract_elems:
                abstract_text = self.extract_element_text(abstract)
                if abstract_text:
                    abstract_texts.append(abstract_text)
            abstract = '\n\n'.join(abstract_texts)
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É
            body_elem = root.find('.//body')
            full_text = ""
            if body_elem is not None:
                full_text = self.extract_element_text(body_elem)
            
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
            kwd_elems = root.findall('.//kwd')
            keywords = [self.extract_element_text(kwd) for kwd in kwd_elems if self.extract_element_text(kwd)]
            
            return {
                'title': self.clean_text(title),
                'abstract': self.clean_text(abstract),
                'full_text': self.clean_text(full_text),
                'keywords': '; '.join(keywords)
            }
            
        except ET.ParseError:
            # –†–µ–∑–µ—Ä–≤–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö XML
            return self.parse_xml_fallback(xml_content)
        except Exception as e:
            print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É: {e}")
            return None
    
    def parse_xml_fallback(self, xml_content):
        """–†–µ–∑–µ—Ä–≤–Ω–∏–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥—É XML"""
        try:
            # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ç–µ–≥—ñ–≤
            text = re.sub(r'<[^>]+>', ' ', xml_content)
            text = re.sub(r'\s+', ' ', text)
            text = html.unescape(text)
            text = self.clean_text(text)
            
            return {
                'title': '',
                'abstract': '',
                'full_text': text,
                'keywords': ''
            }
        except Exception:
            return None
    
    def clean_text(self, text):
        """–û—á–∏—â–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É"""
        if not text:
            return ""
        
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\s+([.,;:!?])', r'\1', text)
        text = re.sub(r'([.,;:!?])\s+', r'\1 ', text)
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        
        return text.strip()
    
    def is_valid_article(self, article_data, min_text_length=20000):
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —Å—Ç–∞—Ç—Ç—è —î –≤–∞–ª–∏–¥–Ω–æ—é (–Ω–µ –æ–±–º–µ–∂–µ–Ω–∞)"""
        if not article_data:
            return False
        
        full_text = article_data.get('full_text', '')
        
        # –Ø–∫—â–æ —Ç–µ–∫—Å—Ç –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π, –π–º–æ–≤—ñ—Ä–Ω–æ —Å—Ç–∞—Ç—Ç—è –æ–±–º–µ–∂–µ–Ω–∞
        if len(full_text) < min_text_length:
            return False
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –æ—Å–Ω–æ–≤–Ω–∏—Ö —Ä–æ–∑–¥—ñ–ª—ñ–≤
        if not any(word in full_text.lower() for word in ['method', 'result', 'discussion', 'conclusion', 'introduction']):
            return False
        
        return True
    
    def save_to_csv(self, results, output_file):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É CSV —Ñ–∞–π–ª"""
        try:
            # –°—Ç–≤–æ—Ä—é—î–º–æ DataFrame
            df_data = []
            for result in results:
                if result['status'] == 'SUCCESS':
                    row = {
                        'Original_Title': result['original_name'],
                        'URL': result['original_url'],
                        'PMC_ID': result['pmc_id'],
                        'Article_Title': result['title'],
                        'Authors': result['authors'],
                        'Journal': result['journal'],
                        'Publication_Date': result['pub_date'],
                        'DOI': result['doi'],
                        'Keywords': result.get('keywords', ''),
                        'Abstract': result['abstract'],
                        'Full_Text': result['full_text'],
                        'Text_Length': len(result['full_text']),
                        'Download_Date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    df_data.append(row)
            
            if not df_data:
                print("‚ùå –ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è")
                return False
            
            df = pd.DataFrame(df_data)
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —É CSV –∑ –∫–æ–¥—É–≤–∞–Ω–Ω—è–º UTF-8
            df.to_csv(output_file, index=False, encoding='utf-8')
            
            return True
            
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è CSV: {e}")
            return False
    
    def save_full_text_files(self, results, output_dir):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø–æ–≤–Ω–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤ —É –æ–∫—Ä–µ–º—ñ —Ñ–∞–π–ª–∏"""
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            for result in results:
                if result['status'] == 'SUCCESS':
                    pmc_id = result['pmc_id']
                    filename = os.path.join(output_dir, f"{pmc_id}_full_text.txt")
                    
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(f"PMC ID: {pmc_id}\n")
                        f.write(f"Title: {result['title']}\n")
                        f.write(f"Authors: {result['authors']}\n")
                        f.write(f"Journal: {result['journal']}\n")
                        f.write(f"Publication Date: {result['pub_date']}\n")
                        f.write(f"DOI: {result['doi']}\n")
                        f.write("="*80 + "\n\n")
                        f.write("ABSTRACT:\n")
                        f.write(result['abstract'])
                        f.write("\n\n" + "="*80 + "\n\n")
                        f.write("FULL TEXT:\n")
                        f.write(result['full_text'])
            
            return True
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö —Ñ–∞–π–ª—ñ–≤: {e}")
            return False
    
    def process_csv_file(self, input_csv, output_csv, text_files_dir=None):
        """–û—Å–Ω–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –æ–±—Ä–æ–±–∫–∏ CSV —Ñ–∞–π–ª—É"""
        try:
            print(f"üìñ –ß–∏—Ç–∞–Ω–Ω—è CSV —Ñ–∞–π–ª—É: {input_csv}")
            df = pd.read_csv(input_csv)
            
            if len(df.columns) < 2:
                print("‚ùå –§–∞–π–ª –º–∞—î –º–µ–Ω—à–µ 2 –∫–æ–ª–æ–Ω–æ–∫")
                return False
            
            name_column = df.columns[0]
            url_column = df.columns[1]
            
            print(f"üîç –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –∫–æ–ª–æ–Ω–∫–∏: '{name_column}' (–Ω–∞–∑–≤–∏), '{url_column}' (–ø–æ—Å–∏–ª–∞–Ω–Ω—è)")
            
            results = []
            
            for index, row in df.iterrows():
                self.stats['total_processed'] += 1
                
                article_name = str(row[name_column]) if pd.notna(row[name_column]) else "–ë–µ–∑ –Ω–∞–∑–≤–∏"
                article_url = str(row[url_column]) if pd.notna(row[url_column]) else ""
                
                print(f"\nüìÑ –û–±—Ä–æ–±–∫–∞ {index + 1}/{len(df)}: {article_name[:60]}...")
                print(f"   üîó URL: {article_url}")
                
                # –í–∏—Ç—è–≥–Ω–µ–Ω–Ω—è PMC ID
                pmc_id = self.extract_pmc_id(article_url)
                
                if not pmc_id:
                    print("   ‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏—Ç—è–≥—Ç–∏ PMC ID")
                    self.stats['failed'] += 1
                    continue
                
                print(f"   ‚úÖ PMC ID: {pmc_id}")
                
                # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö
                print("   üìä –û—Ç—Ä–∏–º–∞–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö...")
                metadata = self.get_article_metadata(pmc_id)
                
                # –û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É —á–µ—Ä–µ–∑ API
                print("   üì° –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É...")
                xml_content = self.get_article_via_api(pmc_id)
                
                if not xml_content:
                    print("   ‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–∞—Ç—Ç—é")
                    self.stats['failed'] += 1
                    continue
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –æ–±–º–µ–∂–µ–Ω—å –¥–æ—Å—Ç—É–ø—É
                if self.check_article_restrictions(xml_content):
                    print("   ‚è≠Ô∏è  –°—Ç–∞—Ç—Ç—è –æ–±–º–µ–∂–µ–Ω–∞ –≤–∏–¥–∞–≤—Ü–µ–º - –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ")
                    self.stats['restricted_skipped'] += 1
                    continue
                
                # –ü–∞—Ä—Å–∏–Ω–≥ –≤–º—ñ—Å—Ç—É —Å—Ç–∞—Ç—Ç—ñ
                print("   üîç –ü–∞—Ä—Å–∏–Ω–≥ –≤–º—ñ—Å—Ç—É...")
                content_data = self.parse_article_content(xml_content, pmc_id)
                
                if not content_data:
                    print("   ‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ –≤–º—ñ—Å—Ç")
                    self.stats['failed'] += 1
                    continue
                
                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ —Å—Ç–∞—Ç—Ç—è —î –≤–∞–ª–∏–¥–Ω–æ—é (–Ω–µ –æ–±–º–µ–∂–µ–Ω–∞)
                if not self.is_valid_article(content_data):
                    print("   ‚è≠Ô∏è  –°—Ç–∞—Ç—Ç—è –º–∞—î –æ–±–º–µ–∂–µ–Ω–∏–π –¥–æ—Å—Ç—É–ø –∞–±–æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ —Ç–µ–∫—Å—Ç—É - –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ")
                    self.stats['restricted_skipped'] += 1
                    continue
                
                # –û–±'—î–¥–Ω—É—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ —Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç
                article_result = {
                    **metadata,
                    **content_data,
                    'original_name': article_name,
                    'original_url': article_url,
                    'pmc_id': pmc_id,
                    'status': 'SUCCESS'
                }
                
                results.append(article_result)
                self.stats['successful'] += 1
                
                print(f"   ‚úÖ –£—Å–ø—ñ—à–Ω–æ! –¢–µ–∫—Å—Ç: {len(content_data['full_text']):,} —Å–∏–º–≤–æ–ª—ñ–≤")
                
                # –î–æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ª—ñ–º—ñ—Ç—ñ–≤ NCBI
                time.sleep(self.delay)
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É CSV
            print(f"\nüíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É CSV...")
            if self.save_to_csv(results, output_csv):
                print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É: {output_csv}")
            else:
                print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏")
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö —Ñ–∞–π–ª—ñ–≤ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            if text_files_dir:
                print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö —Ñ–∞–π–ª—ñ–≤...")
                if self.save_full_text_files(results, text_files_dir):
                    print(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ñ —Ñ–∞–π–ª–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É: {text_files_dir}")
            
            self.print_statistics()
            
            return True
            
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
            return False
    
    def print_statistics(self):
        """–í–∏–≤—ñ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–æ–±–∫–∏"""
        print("\n" + "="*60)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–û–ë–ö–ò:")
        print(f"   üìÑ –í—Å—å–æ–≥–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {self.stats['total_processed']}")
        print(f"   ‚úÖ –£—Å–ø—ñ—à–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {self.stats['successful']}")
        print(f"   ‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–æ–±–º–µ–∂–µ–Ω–∏–π –¥–æ—Å—Ç—É–ø): {self.stats['restricted_skipped']}")
        print(f"   ‚ùå –ü–æ–º–∏–ª–æ–∫: {self.stats['failed']}")
        
        if self.stats['successful'] > 0:
            success_rate = (self.stats['successful'] / self.stats['total_processed']) * 100
            print(f"   üìà –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å: {success_rate:.1f}%")


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""
    extractor = NCBIArticleExtractor(
        email="oleksiyvolkov2435@gmail.com",  # –ó–∞–º—ñ–Ω—ñ—Ç—å –Ω–∞ –≤–∞—à email
        max_retries=3,
        delay=0.34
    )
    
    success = extractor.process_csv_file(
        input_csv='SB_publication_PMC.csv',           # –í–∞—à CSV —Ñ–∞–π–ª
        output_csv='articles_results.csv',  # –í–∏—Ö—ñ–¥–Ω–∏–π CSV —Ñ–∞–π–ª
        text_files_dir='full_texts'     # –ü–∞–ø–∫–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö —Ñ–∞–π–ª—ñ–≤ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
    )
    
    if success:
        print("\nüéâ –û–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ!")
        print("üìù –¢—ñ–ª—å–∫–∏ —Å—Ç–∞—Ç—Ç—ñ –∑ –ø–æ–≤–Ω–∏–º –¥–æ—Å—Ç—É–ø–æ–º –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É CSV —Ñ–∞–π–ª")
        print("üíæ –ü–æ–≤–Ω—ñ —Ç–µ–∫—Å—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –±–µ–∑ –æ–±—Ä—ñ–∑–∞–Ω–Ω—è")
    else:
        print("\n‚ùå –û–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑ –ø–æ–º–∏–ª–∫–∞–º–∏")


if __name__ == "__main__":
    main()